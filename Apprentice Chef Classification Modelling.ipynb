{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianpaulus/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\"\"\" \n",
    "    Apprentice Chef Case\n",
    "    Regression Model Building for DAT-5304 Machine Learning.\n",
    "    The course is part of the MS. in Business Analytics at HULT International\n",
    "    Business School.\n",
    "    \n",
    "    Author: Maximilian Paulus\n",
    "    Submission Date: February 5th, 2020\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "DocString:\n",
    "    a) Introduction:\n",
    "    This Document contains a streamlined version of the exploratory data analysis,\n",
    "    feature engineering, model building, selection and tuning, that was performed\n",
    "    within the scope of predicting cross sales success from Apprentice Chef Customers.\n",
    "    \n",
    "    b) Known Errors or Bugs:\n",
    "        -\n",
    "\"\"\"\n",
    "# Importing Required Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf # linear regression (statsmodels)\n",
    "import sklearn.linear_model # linear models\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import train_test_split # train/test split\n",
    "from sklearn.linear_model import LinearRegression # linear regression (scikit-learn)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "from sklearn.linear_model import RidgeClassifier  # ridge classifier\n",
    "from sklearn.linear_model import Perceptron  # Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier  # Passive Aggressive Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "\n",
    "# CART model packages\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import export_graphviz             # exports graphics\n",
    "from sklearn.externals.six import StringIO           # saves objects in memory\n",
    "from IPython.display import Image                    # displays on frontend\n",
    "import pydotplus                                     # interprets dot objects\n",
    "\n",
    "\n",
    "# new packages\n",
    "from sklearn.model_selection import GridSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer              # customizable scorer\n",
    "\n",
    "########################################\n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800, export = False):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    export     : bool, defalut False\n",
    "        whether or not to export the tree as a .png file\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width,\n",
    "                unconfined = True)\n",
    "\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train_tree.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(25,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')\n",
    "        \n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "########################################\n",
    "# optimal_neighbors\n",
    "########################################\n",
    "def optimal_neighbors(X_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=802,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "X_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 802\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing X_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(X_data)\n",
    "        X_scaled           = scaler.transform(X_data)\n",
    "        X_scaled_df        = pd.DataFrame(X_scaled)\n",
    "        X_data             = X_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Apprentice Chef Dataset\n",
    "filename = 'Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "chef_df = pd.read_excel(filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chef_df.loc[:, :].quantile([0.20,\n",
    "#                           0.40,\n",
    "#                           0.60,\n",
    "#                           0.80,\n",
    "#                           1.00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chef_df['AVG_ORDER_REV'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chef_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values and imputing missing family names with 'n/a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checking for missing values, creating missing value flag and imputing missing family names\n",
    "# chef_df.isna().sum()\n",
    "\n",
    "if chef_df['FAMILY_NAME'].isnull().astype(int).sum() > 0:\n",
    "    chef_df['MISSING_FAMILY_NAME'] = chef_df['FAMILY_NAME'].isnull().astype(int)\n",
    "fill = 'n/a'\n",
    "chef_df['FAMILY_NAME'] = chef_df['FAMILY_NAME'].fillna(fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable for the subsequent analysis is 'CROSS_SELL_SUCCESS'. Looking at the value counts checks for class imbalance (imbalance between success and failure in target variable). The classes in this case are 0 and 1, representing cross sell success and not success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1321\n",
       "0     625\n",
       "Name: CROSS_SELL_SUCCESS, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef_df['CROSS_SELL_SUCCESS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the value counts of 1321 and 625 there is a slight class imbalance underlying that requires stratification of samples when splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting email adresses into address and domain section to classify domains as personal, professional or spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting Email Addresses into address and domain\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in chef_df.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = chef_df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "email_df.columns = ['EMAIL_ADDRESS', 'DOMAIN']\n",
    "chef_df['EMAIL_ADDRESS'] = email_df['EMAIL_ADDRESS']\n",
    "chef_df['DOMAIN'] = email_df['DOMAIN']\n",
    "\n",
    "# displaying the results\n",
    "# chef_df\n",
    "\n",
    "# Classyfing Domains into personal and non-personal\n",
    "\n",
    "personal_mail_list = ['gmail.com', 'yahoo.com', 'protonmail.com']\n",
    "junk_mail_list = ['me.com','aol.com','hotmail.com','live.com','msn.com','passport.com']\n",
    "\n",
    "technology_domain_list = ['apple.com','ibm.com','microsoft.com','verizon.com',\n",
    "                          'unitedtech.com','cisco.com','intel.com']\n",
    "financial_domain_list = ['amex.com','travellers.com','visa.com','jpmorgan.com'\n",
    "                         ,'goldmansacs.com']\n",
    "\n",
    "############################################\n",
    "# The industry specific domain lists are extracted from \n",
    "# clearbit enrichment data on the existing professional domains\n",
    "# https://docs.google.com/spreadsheets/d/1erIdqoy60JwLAnpb91EfoJV5YrXDnbwSaA-aqcBlw48/edit#gid=1561611259\n",
    "# The industry naming is changed for convenience purposes\n",
    "############################################\n",
    "\n",
    "\n",
    "# looping over the domain column to identify personal domains\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'DOMAIN'] in personal_mail_list:\n",
    "        chef_df.loc[index, 'IS_PERSONAL'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'IS_PERSONAL'] = int(0)\n",
    "chef_df['IS_PERSONAL'] = chef_df['IS_PERSONAL'].astype('int64')       \n",
    "\n",
    "# looping over the domain column to identify spam / junk domains\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'DOMAIN'] in junk_mail_list:\n",
    "        chef_df.loc[index, 'IS_SPAM'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'IS_SPAM'] = int(0)\n",
    "chef_df['IS_SPAM'] = chef_df['IS_SPAM'].astype('int64')  \n",
    "\n",
    "# looping over the domain column to classify domains that are not junk or personal as professional\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'DOMAIN'] not in junk_mail_list and chef_df.loc[index, 'DOMAIN'] not in personal_mail_list:\n",
    "        chef_df.loc[index, 'IS_PROFESSIONAL'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'IS_PROFESSIONAL'] = int(0)\n",
    "chef_df['IS_PROFESSIONAL'] = chef_df['IS_PROFESSIONAL'].astype('int64')         \n",
    "  \n",
    "    \n",
    "# looping over the domain column to classify domains from financial industry\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'DOMAIN'] in financial_domain_list:\n",
    "        chef_df.loc[index, 'IS_FINANCIAL'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'IS_FINANCIAL'] = int(0)\n",
    "chef_df['IS_FINANCIAL'] = chef_df['IS_FINANCIAL'].astype('int64')         \n",
    "        \n",
    "# looping over the domain column to classify domains from tech industry\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'DOMAIN'] in technology_domain_list:\n",
    "        chef_df.loc[index, 'IS_TECH'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'IS_TECH'] = int(0)\n",
    "chef_df['IS_TECH'] = chef_df['IS_TECH'].astype('int64')         \n",
    "            \n",
    "    \n",
    "    \n",
    "# Checking the profesional domains for wrongly categorized domains\n",
    "#chef_df['DOMAIN'][chef_df['IS_PROFESSIONAL'] == 1].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Visual EDA (From Assignment 1). We take the same outlier thresholds for the explanatory variables that we defined in Assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Visual EDA (Histograms)\n",
    "########################\n",
    "# Setting Outlier Thresholds\n",
    "\n",
    "total_meals_hi = 150\n",
    "total_meals_lo = 0\n",
    "unique_meals_hi = 10\n",
    "customer_service_hi = 8\n",
    "avg_time_hi = 200\n",
    "cancellations_before_hi = 5\n",
    "weekly_plan_hi = 20\n",
    "prep_vid_time_hi = 250\n",
    "late_deliveries_hi = 7\n",
    "early_deliveries_hi = 1\n",
    "master_classes_hi = 2\n",
    "avg_clicks_lo = 8\n",
    "avg_clicks_hi = 19\n",
    "total_photos_hi = 1\n",
    "\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(chef_df['TOTAL_MEALS_ORDERED'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'orange')\n",
    "#plt.xlabel('TOTAL_MEALS_ORDERED')\n",
    "#plt.axvline(x = total_meals_hi)\n",
    "#plt.axvline(x = total_meals_lo)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 2)\n",
    "#sns.distplot(chef_df['UNIQUE_MEALS_PURCH'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'r')\n",
    "#plt.xlabel('UNIQUE_MEALS_PURCH')\n",
    "#plt.axvline(x = unique_meals_hi)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 3)\n",
    "#sns.distplot(chef_df['CONTACTS_W_CUSTOMER_SERVICE'],\n",
    "#             bins  = 'fd',\n",
    "#             color = 'g')\n",
    "#plt.xlabel('CONTACTS_W_CUSTOMER_SERVICE')\n",
    "#plt.axvline(x = customer_service_hi)\n",
    "\n",
    "########################\n",
    "#plt.subplot(2, 2, 4)\n",
    "#sns.distplot(chef_df['AVG_TIME_PER_SITE_VISIT'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'orange')\n",
    "#plt.axvline(x = avg_time_hi)\n",
    "#plt.xlabel('AVG_TIME_PER_SITE_VISIT')\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('Apprentice Chef Final Histograms 1 of 3.png')\n",
    "#plt.show()\n",
    "\n",
    "########################\n",
    "########################\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(chef_df['CANCELLATIONS_BEFORE_NOON'],\n",
    "#             bins  = 'fd',\n",
    "#             color = 'y')\n",
    "#plt.xlabel('CANCELLATIONS_BEFORE_NOON')\n",
    "#plt.axvline(x = cancellations_before_hi)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 2)\n",
    "#sns.distplot(chef_df['WEEKLY_PLAN'],\n",
    "#             bins  = 'fd',\n",
    "#             color = 'y')\n",
    "#plt.xlabel('WEEKLY_PLAN')\n",
    "#plt.axvline(x = weekly_plan_hi)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 3)\n",
    "#sns.distplot(chef_df['AVG_PREP_VID_TIME'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'r')\n",
    "#plt.xlabel('AVG_PREP_VID_TIME')\n",
    "#plt.axvline(x = prep_vid_time_hi)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 4)\n",
    "#sns.distplot(chef_df['EARLY_DELIVERIES'],\n",
    "#             bins = 10,\n",
    "#             kde  = False,\n",
    "#             rug  = True,\n",
    "#             color = 'orange')\n",
    "#plt.xlabel('EARLY_DELIVERIES')\n",
    "#plt.axvline(x = early_deliveries_hi)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('Apprentice Chef Final Histograms 2 of 3.png')\n",
    "#plt.show()\n",
    "\n",
    "########################\n",
    "########################\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(chef_df['LATE_DELIVERIES'],\n",
    "#             bins  = 'fd',\n",
    "#             color = 'g')\n",
    "#plt.xlabel('LATE_DELIVERIES')\n",
    "#plt.axvline(x = late_deliveries_hi)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 2)\n",
    "#sns.distplot(chef_df['MASTER_CLASSES_ATTENDED'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'r')\n",
    "#plt.xlabel('MASTER_CLASSES_ATTENDED')\n",
    "#plt.axvline(x = master_classes_hi)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 3)\n",
    "#sns.distplot(chef_df['AVG_CLICKS_PER_VISIT'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'r')\n",
    "#plt.xlabel('AVG_CLICKS_PER_VISIT')\n",
    "#plt.axvline(x = avg_clicks_hi)\n",
    "#plt.axvline(x = avg_clicks_lo)\n",
    "\n",
    "########################\n",
    "\n",
    "#plt.subplot(2, 2, 4)\n",
    "#sns.distplot(chef_df['TOTAL_PHOTOS_VIEWED'],\n",
    "#             bins  = 'fd',\n",
    "#             kde   = False,\n",
    "#             rug   = True,\n",
    "#             color = 'r')\n",
    "#plt.xlabel('TOTAL_PHOTOS_VIEWED')\n",
    "#plt.axvline(x = total_photos_hi)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('Apprentice Chef Final Histograms 3 of 3.png')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# developing features (columns) for outliers\n",
    "\n",
    "# Total Meals\n",
    "chef_df['OUT_TOTAL_MEALS_ORDERED'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_TOTAL_MEALS_ORDERED'][chef_df['TOTAL_MEALS_ORDERED'] > total_meals_hi]\n",
    "condition_lo = chef_df.loc[0:,'OUT_TOTAL_MEALS_ORDERED'][chef_df['TOTAL_MEALS_ORDERED'] <= total_meals_lo]\n",
    "\n",
    "chef_df['OUT_TOTAL_MEALS_ORDERED'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "chef_df['OUT_TOTAL_MEALS_ORDERED'].replace(to_replace = condition_lo,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "# Unique Meals\n",
    "chef_df['OUT_UNIQUE_MEALS_PURCH'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_UNIQUE_MEALS_PURCH'][chef_df['UNIQUE_MEALS_PURCH'] > unique_meals_hi]\n",
    "\n",
    "chef_df['OUT_UNIQUE_MEALS_PURCH'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Customer Service\n",
    "chef_df['OUT_CONTACTS_W_CUSTOMER_SERVICE'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_CONTACTS_W_CUSTOMER_SERVICE'][chef_df['CONTACTS_W_CUSTOMER_SERVICE'] > customer_service_hi]\n",
    "\n",
    "chef_df['OUT_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Cancellations before Noon\n",
    "chef_df['OUT_CANCELLATIONS_BEFORE_NOON'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_CANCELLATIONS_BEFORE_NOON'][chef_df['CANCELLATIONS_BEFORE_NOON'] > cancellations_before_hi]\n",
    "\n",
    "chef_df['OUT_CANCELLATIONS_BEFORE_NOON'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Weekly Plan\n",
    "chef_df['OUT_WEEKLY_PLAN'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_WEEKLY_PLAN'][chef_df['WEEKLY_PLAN'] > weekly_plan_hi]\n",
    "\n",
    "chef_df['OUT_WEEKLY_PLAN'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Prep Vid Time\n",
    "chef_df['OUT_AVG_PREP_VID_TIME'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_AVG_PREP_VID_TIME'][chef_df['AVG_PREP_VID_TIME'] > prep_vid_time_hi]\n",
    "\n",
    "chef_df['OUT_AVG_PREP_VID_TIME'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Late Deliveries\n",
    "chef_df['OUT_LATE_DELIVERIES'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_LATE_DELIVERIES'][chef_df['LATE_DELIVERIES'] > late_deliveries_hi]\n",
    "\n",
    "chef_df['OUT_LATE_DELIVERIES'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "# Early Deliveries\n",
    "chef_df['OUT_EARLY_DELIVERIES'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_EARLY_DELIVERIES'][chef_df['EARLY_DELIVERIES'] > early_deliveries_hi]\n",
    "\n",
    "chef_df['OUT_EARLY_DELIVERIES'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Avg Time\n",
    "chef_df['OUT_AVG_TIME_PER_SITE_VISIT'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_AVG_TIME_PER_SITE_VISIT'][chef_df['AVG_TIME_PER_SITE_VISIT'] > avg_time_hi]\n",
    "\n",
    "chef_df['OUT_AVG_TIME_PER_SITE_VISIT'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Master Classes\n",
    "chef_df['OUT_MASTER_CLASSES_ATTENDED'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_MASTER_CLASSES_ATTENDED'][chef_df['MASTER_CLASSES_ATTENDED'] > master_classes_hi]\n",
    "\n",
    "chef_df['OUT_MASTER_CLASSES_ATTENDED'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Average Clicks\n",
    "chef_df['OUT_AVG_CLICKS_PER_VISIT'] = 0\n",
    "condition_hi = chef_df.loc[0:,'OUT_AVG_CLICKS_PER_VISIT'][chef_df['AVG_CLICKS_PER_VISIT'] > avg_clicks_hi]\n",
    "condition_lo = chef_df.loc[0:,'OUT_AVG_CLICKS_PER_VISIT'][chef_df['AVG_CLICKS_PER_VISIT'] < avg_clicks_lo]\n",
    "\n",
    "\n",
    "chef_df['OUT_AVG_CLICKS_PER_VISIT'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "#chef_df['OUT_AVG_CLICKS_PER_VISIT'].replace(to_replace = condition_lo,\n",
    "#                                value      = 1,\n",
    "#                                inplace    = True)\n",
    "\n",
    "# Total Photos\n",
    "#chef_df['OUT_TOTAL_PHOTOS_VIEWED'] = 0\n",
    "#condition_hi = chef_df.loc[0:,'OUT_TOTAL_PHOTOS_VIEWED'][chef_df['TOTAL_PHOTOS_VIEWED'] > total_photos_hi]\n",
    "\n",
    "#chef_df['OUT_TOTAL_PHOTOS_VIEWED'].replace(to_replace = condition_hi,\n",
    "#                                value      = 1,\n",
    "#                                inplace    = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating fields based on existing features. The following new features could be interesting to look at:\n",
    "    \n",
    "    1. Percentage of early deliveries (early deliveries / total deliveries)\n",
    "    2. Percentage of late deliveries (late deliveries / total deliveries)\n",
    "    3. Did the customer ever give a rating? (median_rating > 0)\n",
    "    4. Percentage of meals from weekly plan (assuming basic weekly plan = 3 meals)\n",
    "    5. Did customer attend cooking class? (0 and 1 encoding)\n",
    "    6. Average meals per month\n",
    "    7. Average revenue per meal\n",
    "    8. Flag Follower customers (people that follow 35 % or more of their meal recommendations\n",
    "    9. Likely ordered drinks\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1: percentage of early deliveries\n",
    "for index, col in chef_df.iterrows():\n",
    "    chef_df.loc[index,'PCT_EARLY_DELIVERIES'] = chef_df.loc[index,'EARLY_DELIVERIES'] / chef_df.loc[index,'TOTAL_MEALS_ORDERED']\n",
    "\n",
    "    \n",
    "# Feature 2:\n",
    "for index, col in chef_df.iterrows():\n",
    "    chef_df.loc[index,'PCT_LATE_DELIVERIES'] = chef_df.loc[index,'LATE_DELIVERIES'] / chef_df.loc[index,'TOTAL_MEALS_ORDERED']\n",
    "\n",
    "    \n",
    "# Feature 3: did the customer ever give a rating?\n",
    "#for index, col in chef_df.iterrows():\n",
    "#    if chef_df.loc[index, 'MEDIAN_MEAL_RATING'] > 0:\n",
    "#        chef_df.loc[index, 'EVER_RATED'] = int(1)\n",
    "#    else:\n",
    "#        chef_df.loc[index, 'EVER_RATED'] = int(0)\n",
    "\n",
    "\n",
    "# Feature 4: percentage of meals from weekly plan\n",
    "for index, col in chef_df.iterrows():\n",
    "    chef_df.loc[index,'PCT_WEEKLY_PLAN'] = chef_df.loc[index,'WEEKLY_PLAN']*3 / chef_df.loc[index,'TOTAL_MEALS_ORDERED']\n",
    "\n",
    "\n",
    "# Feature 5: did the customer attend cooking class?\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'MASTER_CLASSES_ATTENDED'] > 0:\n",
    "        chef_df.loc[index, 'ATTENDED_CLASS'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'ATTENDED_CLASS'] = int(0)\n",
    "    \n",
    "chef_df['ATTENDED_CLASS'] = chef_df['ATTENDED_CLASS'].astype('int64')  \n",
    "\n",
    "\n",
    "# Feature 6: average meals per month\n",
    "for index, col in chef_df.iterrows():\n",
    "    chef_df.loc[index,'AVG_MEALS_MONTH'] = chef_df.loc[index,'TOTAL_MEALS_ORDERED'] / 12\n",
    "    \n",
    "# Feature 7: avg price per order\n",
    "for index, col in chef_df.iterrows():\n",
    "    chef_df.loc[index,'AVG_ORDER_REV'] = chef_df.loc[index,'REVENUE'] / chef_df.loc[index,'TOTAL_MEALS_ORDERED']\n",
    "    \n",
    "        \n",
    "# Feature 8: is_follower\n",
    "for index, col in chef_df.iterrows():\n",
    "    if chef_df.loc[index, 'FOLLOWED_RECOMMENDATIONS_PCT'] > 35:\n",
    "        chef_df.loc[index, 'IS_FOLLOWER'] = int(1)\n",
    "    else:\n",
    "        chef_df.loc[index, 'IS_FOLLOWER'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chef_df_corr_cross = chef_df.corr()\n",
    "# chef_df_corr_cross['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for val in chef_data:\n",
    "#    print(f\"{val} +\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following logistic regression model is the result of multiple iterations. The remaining variables are considered significant are used in the modeling process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating explanatory and target variable for modelling\n",
    "chef_data_full =  chef_df.loc[:, ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH',\n",
    "       'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "       'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON',\n",
    "       'CANCELLATIONS_AFTER_NOON', 'TASTES_AND_PREFERENCES', 'MOBILE_LOGINS',\n",
    "       'PC_LOGINS', 'WEEKLY_PLAN', 'EARLY_DELIVERIES', 'LATE_DELIVERIES',\n",
    "       'PACKAGE_LOCKER', 'REFRIGERATED_LOCKER', 'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "       'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'MASTER_CLASSES_ATTENDED',\n",
    "       'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED',\n",
    "       'MISSING_FAMILY_NAME','IS_SPAM', 'IS_PROFESSIONAL', 'PCT_EARLY_DELIVERIES',\n",
    "       'PCT_LATE_DELIVERIES', 'PCT_WEEKLY_PLAN','IS_FINANCIAL','IS_TECH',\n",
    "       'ATTENDED_CLASS', 'AVG_MEALS_MONTH', 'AVG_ORDER_REV',\n",
    "       'IS_FOLLOWER']]\n",
    "                              \n",
    "chef_target_full = chef_df.loc[:,'CROSS_SELL_SUCCESS']\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(chef_data_full, chef_target_full, \n",
    "                                                    test_size = 0.25, random_state = 822, \n",
    "                                                    stratify = chef_target_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.443816\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>CROSS_SELL_SUCCESS</td> <th>  No. Observations:  </th>   <td>  1459</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Logit</td>       <th>  Df Residuals:      </th>   <td>  1448</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>        <th>  Df Model:          </th>   <td>    10</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 05 Feb 2020</td>  <th>  Pseudo R-squ.:     </th>   <td>0.2932</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:35:13</td>      <th>  Log-Likelihood:    </th>  <td> -647.53</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>True</td>        <th>  LL-Null:           </th>  <td> -916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>  LLR p-value:       </th> <td>4.605e-109</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                    <td>   -1.3404</td> <td>    0.319</td> <td>   -4.201</td> <td> 0.000</td> <td>   -1.966</td> <td>   -0.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_NUMBER</th>                <td>    0.7582</td> <td>    0.199</td> <td>    3.817</td> <td> 0.000</td> <td>    0.369</td> <td>    1.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_BEFORE_NOON</th>    <td>    0.2438</td> <td>    0.049</td> <td>    4.960</td> <td> 0.000</td> <td>    0.147</td> <td>    0.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_AFTER_NOON</th>     <td>   -0.2844</td> <td>    0.152</td> <td>   -1.868</td> <td> 0.062</td> <td>   -0.583</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TASTES_AND_PREFERENCES</th>       <td>    0.5127</td> <td>    0.148</td> <td>    3.466</td> <td> 0.001</td> <td>    0.223</td> <td>    0.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PC_LOGINS</th>                    <td>   -0.4559</td> <td>    0.131</td> <td>   -3.468</td> <td> 0.001</td> <td>   -0.714</td> <td>   -0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FOLLOWED_RECOMMENDATIONS_PCT</th> <td>    0.0565</td> <td>    0.004</td> <td>   14.486</td> <td> 0.000</td> <td>    0.049</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MISSING_FAMILY_NAME</th>          <td>    1.4252</td> <td>    0.477</td> <td>    2.986</td> <td> 0.003</td> <td>    0.490</td> <td>    2.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IS_SPAM</th>                      <td>   -1.2013</td> <td>    0.176</td> <td>   -6.824</td> <td> 0.000</td> <td>   -1.546</td> <td>   -0.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IS_PROFESSIONAL</th>              <td>    0.4483</td> <td>    0.171</td> <td>    2.616</td> <td> 0.009</td> <td>    0.112</td> <td>    0.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IS_TECH</th>                      <td>    0.6316</td> <td>    0.318</td> <td>    1.988</td> <td> 0.047</td> <td>    0.009</td> <td>    1.255</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:     CROSS_SELL_SUCCESS   No. Observations:                 1459\n",
       "Model:                          Logit   Df Residuals:                     1448\n",
       "Method:                           MLE   Df Model:                           10\n",
       "Date:                Wed, 05 Feb 2020   Pseudo R-squ.:                  0.2932\n",
       "Time:                        23:35:13   Log-Likelihood:                -647.53\n",
       "converged:                       True   LL-Null:                       -916.19\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.605e-109\n",
       "================================================================================================\n",
       "                                   coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------\n",
       "Intercept                       -1.3404      0.319     -4.201      0.000      -1.966      -0.715\n",
       "MOBILE_NUMBER                    0.7582      0.199      3.817      0.000       0.369       1.148\n",
       "CANCELLATIONS_BEFORE_NOON        0.2438      0.049      4.960      0.000       0.147       0.340\n",
       "CANCELLATIONS_AFTER_NOON        -0.2844      0.152     -1.868      0.062      -0.583       0.014\n",
       "TASTES_AND_PREFERENCES           0.5127      0.148      3.466      0.001       0.223       0.803\n",
       "PC_LOGINS                       -0.4559      0.131     -3.468      0.001      -0.714      -0.198\n",
       "FOLLOWED_RECOMMENDATIONS_PCT     0.0565      0.004     14.486      0.000       0.049       0.064\n",
       "MISSING_FAMILY_NAME              1.4252      0.477      2.986      0.003       0.490       2.361\n",
       "IS_SPAM                         -1.2013      0.176     -6.824      0.000      -1.546      -0.856\n",
       "IS_PROFESSIONAL                  0.4483      0.171      2.616      0.009       0.112       0.784\n",
       "IS_TECH                          0.6316      0.318      1.988      0.047       0.009       1.255\n",
       "================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "                                    MOBILE_NUMBER +\n",
    "                                    CANCELLATIONS_BEFORE_NOON +\n",
    "                                    CANCELLATIONS_AFTER_NOON +\n",
    "                                    TASTES_AND_PREFERENCES +\n",
    "                                    PC_LOGINS +\n",
    "                                    FOLLOWED_RECOMMENDATIONS_PCT +\n",
    "                                    MISSING_FAMILY_NAME +\n",
    "                                    IS_SPAM +\n",
    "                                    IS_PROFESSIONAL + \n",
    "                                    IS_TECH\n",
    "                                    \"\"\",\n",
    "                                     data    = pd.concat([X_train_full, y_train_full], axis = 1))\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_full = logistic_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the logistic regressions are used to classify features as significant.\n",
    "\n",
    "During iteration, we have seen that the variable \"IS_FOLLOWER\" has a very high coefficient. IS_FOLLOWER and IS_TECH in combination is specified as variable set \"short\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory sets from last session\n",
    "\n",
    "# creating a dictionary to store candidate models\n",
    "\n",
    "feature_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH',\n",
    "       'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "       'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON',\n",
    "       'CANCELLATIONS_AFTER_NOON', 'TASTES_AND_PREFERENCES', 'MOBILE_LOGINS',\n",
    "       'PC_LOGINS', 'WEEKLY_PLAN', 'EARLY_DELIVERIES', 'LATE_DELIVERIES',\n",
    "       'PACKAGE_LOCKER', 'REFRIGERATED_LOCKER', 'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "       'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'MASTER_CLASSES_ATTENDED',\n",
    "       'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED',\n",
    "       'MISSING_FAMILY_NAME','IS_SPAM', 'IS_PROFESSIONAL', 'PCT_EARLY_DELIVERIES',\n",
    "       'PCT_LATE_DELIVERIES', 'PCT_WEEKLY_PLAN','IS_FINANCIAL','IS_TECH',\n",
    "       'ATTENDED_CLASS', 'AVG_MEALS_MONTH', 'AVG_ORDER_REV',\n",
    "       'IS_FOLLOWER'],\n",
    " \n",
    " # significant variables only\n",
    " 'logit_sig'    : ['CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON',\n",
    "                   'TASTES_AND_PREFERENCES',\n",
    "                  'MISSING_FAMILY_NAME','IS_SPAM','IS_PROFESSIONAL','IS_FOLLOWER','IS_TECH'],\n",
    "    \n",
    "  # variables selected through feature importance graph (tuned tree)\n",
    " 'logit_sig_tree' : ['IS_SPAM','IS_FOLLOWER','IS_PROFESSIONAL','CANCELLATIONS_BEFORE_NOON',\n",
    "                     'AVG_PREP_VID_TIME','MOBILE_NUMBER','PCT_WEEKLY_PLAN','AVG_TIME_PER_SITE_VISIT',\n",
    "                     'AVG_CLICKS_PER_VISIT','AVG_ORDER_REV']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides looking at the correlation and significance of individual explanatory variables with our target variable (\"CROSS_SALE_SUCCESS\") we can gain additional insights from building a classification tree. The splits that are chosen by the tree model can give valuable insights into trends that can be featured out in separate variables.\n",
    "\n",
    "\n",
    "Using gridsearch, we tune the Hyperparameter of a DecisionTreeClassifier to keep the tree at a reasonable and interpretable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "\n",
    "# max_depth_space  = pd.np.arange(1, 10, 1)\n",
    "# splitter_space = ['best','random']\n",
    "# min_samples_leaf_space = pd.np.arange(1, 100, 1)\n",
    "# criterion_space = ['gini','entropy']\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "# param_grid = {'max_depth'          : max_depth_space,\n",
    "#               'min_samples_leaf'   : min_samples_leaf_space,\n",
    "#               'criterion'          : criterion_space,\n",
    "#               'splitter'           : splitter_space}\n",
    "\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "# class_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "# class_tree_cv = GridSearchCV(estimator  = class_tree,\n",
    "#                            param_grid = param_grid,\n",
    "#                            cv         = 3,\n",
    "#                            scoring    = make_scorer(roc_auc_score,\n",
    "#                                                    needs_threshold = False))\n",
    "\n",
    "# creating explanatory and target variable\n",
    "# chef_data_tree = chef_df.drop(labels = ['CROSS_SELL_SUCCESS','NAME','FAMILY_NAME','FIRST_NAME','EMAIL','DOMAIN','EMAIL_ADDRESS','IS_FOLLOWER'], axis = 1)\n",
    "# chef_target_tree = chef_df.loc[:,'CROSS_SELL_SUCCESS']\n",
    "# X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(chef_data_tree, chef_target_tree, \n",
    "#                                                     test_size = 0.25, random_state = 822, \n",
    "#                                                     stratify = chef_target_tree)\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "# class_tree_cv.fit(chef_data_tree, chef_target_tree)\n",
    "\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", class_tree_cv.best_params_)\n",
    "# print(\"Tuned CV AUC      :\", class_tree_cv.best_score_.round(4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning results in the following optimal parameters: \n",
    "'criterion': 'entropy', 'max_depth': 7, 'min_samples_leaf': 37, 'splitter': 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8053\n",
      "Testing  ACCURACY: 0.7988\n",
      "AUC Score        : 0.7655\n"
     ]
    }
   ],
   "source": [
    "# creating explanatory and target variable\n",
    "chef_data_tree = chef_df.drop(labels = ['CROSS_SELL_SUCCESS','NAME','FAMILY_NAME','FIRST_NAME','EMAIL','DOMAIN','EMAIL_ADDRESS','IS_FOLLOWER'], axis = 1)\n",
    "chef_target_tree = chef_df.loc[:,'CROSS_SELL_SUCCESS']\n",
    "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(chef_data_tree, chef_target_tree, \n",
    "                                                    test_size = 0.25, random_state = 822, \n",
    "                                                    stratify = chef_target_tree)\n",
    "\n",
    "\n",
    "# INSTANTIATING a classification tree model with tuned values\n",
    "class_tree_tuned = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                                          max_depth = 7,\n",
    "                                         min_samples_leaf = 37,\n",
    "                                         splitter = 'best')\n",
    "\n",
    "class_tree_tuned_fit = class_tree_tuned.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "class_tree_tuned_pred = class_tree_tuned_fit.predict(X_test_tree)\n",
    "\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', class_tree_tuned.score(X_train_tree, y_train_tree).round(4))\n",
    "print('Testing  ACCURACY:', class_tree_tuned.score(X_test_tree, y_test_tree).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_tree, y_score = class_tree_tuned_pred).round(4))\n",
    "\n",
    "# displaying the tree\n",
    "#display_tree(tree       = class_tree_tuned,\n",
    "#             feature_df = X_train_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of the accuracy measurements, we also look into the confusion matrix. \n",
    "In our case we need to look at two cases:\n",
    "    1. We predict a customer to buy \"halfway there\" but the customer doesn't (false positive)\n",
    "    2. We predict a customer not to buy \"halfway there\" but the customer buys \"halfway there\" (false negative)\n",
    "    \n",
    "When launching a new product, we don't want to miss out on sales opportunities and rather offer the product to a couple non interested customers. Hence, it is better to accept a higher number of false positives than false negatives. \n",
    "\n",
    "\n",
    "The following model selection will always consider the number of false negatives (number of missed opportunities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_tree,\n",
    "#          pred_y = class_tree_tuned_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the tree performance into a dataframe that will store all results of subsequent modelling attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Tree Tuned</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model Training Accuracy Testing Accuracy AUC Value  \\\n",
       "1  Classification Tree Tuned            0.8053           0.7988    0.7655   \n",
       "\n",
       "  False Positives False Negatives  \n",
       "1              47              51  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df = [['Model', 'Training Accuracy','Testing Accuracy', 'AUC Value',\n",
    "                      'False Positives',' False Negatives']]\n",
    "# saving the results\n",
    "performance_df.append(['Classification Tree Tuned',\n",
    "                          class_tree_tuned.score(X_train_tree, y_train_tree).round(4),\n",
    "                          class_tree_tuned.score(X_test_tree, y_test_tree).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_tree, y_score = class_tree_tuned_pred).round(4),\n",
    "                         47,\n",
    "                         51])\n",
    "\n",
    "performance_df = pd.DataFrame(performance_df)\n",
    "performance_df.columns = ['Model', 'Training Accuracy','Testing Accuracy', 'AUC Value',\n",
    "                      'False Positives','False Negatives']\n",
    "performance_df.drop(index = 0, axis = 0,inplace = True)\n",
    "\n",
    "\n",
    "# saving the DataFrame to Excel\n",
    "performance_df.to_excel('Classification Model Performance.xlsx',\n",
    "                              index = False)\n",
    "\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importances can be helpful to select features during the further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FOLLOWED_RECOMMENDATIONS_PCT</td>\n",
       "      <td>0.816821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IS_SPAM</td>\n",
       "      <td>0.067952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IS_PROFESSIONAL</td>\n",
       "      <td>0.019042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AVG_TIME_PER_SITE_VISIT</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CANCELLATIONS_BEFORE_NOON</td>\n",
       "      <td>0.016505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AVG_PREP_VID_TIME</td>\n",
       "      <td>0.012248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PCT_WEEKLY_PLAN</td>\n",
       "      <td>0.011104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PC_LOGINS</td>\n",
       "      <td>0.009949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LATE_DELIVERIES</td>\n",
       "      <td>0.008445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNIQUE_MEALS_PURCH</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOTAL_MEALS_ORDERED</td>\n",
       "      <td>0.006139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>AVG_ORDER_REV</td>\n",
       "      <td>0.005761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Feature  Importance\n",
       "17  FOLLOWED_RECOMMENDATIONS_PCT    0.816821\n",
       "26                       IS_SPAM    0.067952\n",
       "27               IS_PROFESSIONAL    0.019042\n",
       "5        AVG_TIME_PER_SITE_VISIT    0.018515\n",
       "7      CANCELLATIONS_BEFORE_NOON    0.016505\n",
       "18             AVG_PREP_VID_TIME    0.012248\n",
       "43               PCT_WEEKLY_PLAN    0.011104\n",
       "11                     PC_LOGINS    0.009949\n",
       "14               LATE_DELIVERIES    0.008445\n",
       "2             UNIQUE_MEALS_PURCH    0.007519\n",
       "1            TOTAL_MEALS_ORDERED    0.006139\n",
       "46                 AVG_ORDER_REV    0.005761"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.concat([pd.DataFrame(X_train_tree.columns),\n",
    "                                 pd.DataFrame(class_tree_tuned.feature_importances_)],axis = 1)\n",
    "feature_importances.columns = ['Feature','Importance']\n",
    "feature_importances[feature_importances['Importance'] > 0].sort_values(by = 'Importance',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building actual tree model using the optimal parameters and only significant x variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7951\n",
      "Testing  ACCURACY: 0.7885\n",
      "AUC Score        : 0.7766\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chef_data_sig   =  chef_df.loc[ : , feature_dict['logit_sig']]\n",
    "#chef_data_sig   =  chef_df.loc[ : , feature_dict['logit_sig_tree']]\n",
    "#chef_data_short   =  chef_df.loc[ : , ['IS_FOLLOWER','IS_TECH']]\n",
    "chef_target_sig =  chef_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "X_train_sig, X_test_sig, y_train_sig, y_test_sig = train_test_split(\n",
    "            chef_data_sig,\n",
    "            chef_target_sig,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 802,\n",
    "            stratify     = chef_target_sig)\n",
    "\n",
    "\n",
    "\n",
    "# INSTANTIATING a classification tree model with tuned values\n",
    "class_tree_pruned = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                                          max_depth = 7,\n",
    "                                         min_samples_leaf = 37,\n",
    "                                         splitter = 'best')\n",
    "\n",
    "class_tree_pruned_fit = class_tree_pruned.fit(X_train_sig,y_train_sig)\n",
    "# PREDICTING based on the testing set\n",
    "class_tree_pruned_pred = class_tree_pruned_fit.predict(X_test_sig)\n",
    "\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', class_tree_pruned.score(X_train_sig, y_train_sig).round(4))\n",
    "print('Testing  ACCURACY:', class_tree_pruned.score(X_test_sig, y_test_sig).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_sig, y_score = class_tree_pruned_pred).round(4))\n",
    "\n",
    "# displaying the tree\n",
    "#display_tree(tree       = class_tree_pruned,\n",
    "#             feature_df = X_train_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_sig,\n",
    "#          pred_y = class_tree_pruned_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification Tree Tuned</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Tree Pruned</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model Training Accuracy Testing Accuracy AUC Value  \\\n",
       "0   Classification Tree Tuned            0.8053           0.7988    0.7655   \n",
       "1  Classification Tree Pruned            0.7951           0.7885    0.7766   \n",
       "\n",
       "  False Positives False Negatives  \n",
       "0              47              51  \n",
       "1              80              27  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_acc_tree_pruned = class_tree_pruned.score(X_train_sig, y_train_sig).round(4)\n",
    "testing_acc_tree_pruned = class_tree_pruned.score(X_test_sig, y_test_sig).round(4)\n",
    "auc_score_tree_pruned = roc_auc_score(y_true  = y_test_sig, y_score = class_tree_pruned_pred).round(4)\n",
    "false_positives_tree_pruned = 80\n",
    "false_negatives_tree_pruned = 27\n",
    "\n",
    "performance_df = performance_df.append(\n",
    "                          {'Model'             : 'Classification Tree Pruned',\n",
    "                          'Training Accuracy'  : training_acc_tree_pruned,\n",
    "                          'Testing Accuracy'   : testing_acc_tree_pruned,\n",
    "                          'AUC Value'          : auc_score_tree_pruned,\n",
    "                          'False Positives'    : false_positives_tree_pruned,\n",
    "                          'False Negatives'    : false_negatives_tree_pruned},\n",
    "                          ignore_index = True)\n",
    "\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of significant variables is added used to build a logistic regression model with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7978\n",
      "Testing  ACCURACY: 0.7864\n",
      "AUC Score        : 0.7463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chef_data_sig   =  chef_df.loc[ : , feature_dict['logit_sig']]\n",
    "#chef_data_sig   =  chef_df.loc[ : , feature_dict['logit_sig_tree']]\n",
    "#chef_data_short   =  chef_df.loc[ : , ['IS_FOLLOWER','IS_TECH']]\n",
    "chef_target_sig =  chef_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "X_train_sig, X_test_sig, y_train_sig, y_test_sig = train_test_split(\n",
    "            chef_data_sig,\n",
    "            chef_target_sig,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 802,\n",
    "            stratify     = chef_target_sig)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            max_iter = 1000,\n",
    "                            C = 1,\n",
    "                            random_state = 802)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train_sig, y_train_sig)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test_sig)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(X_train_sig, y_train_sig).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(X_test_sig, y_test_sig).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = logreg_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_sig,\n",
    "#          pred_y = logreg_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows the following results:\n",
    "\n",
    "- 53 customers are predicted to buy although they actually didn't. (False Positives)\n",
    "- 51 Customers are predicted to not buy, although they actually did (False Negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification Tree Tuned</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Tree Pruned</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression Significant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model Training Accuracy Testing Accuracy  \\\n",
       "0        Classification Tree Tuned            0.8053           0.7988   \n",
       "1       Classification Tree Pruned            0.7951           0.7885   \n",
       "2  Logistic Regression Significant            0.7978           0.7864   \n",
       "\n",
       "  AUC Value False Positives False Negatives  \n",
       "0    0.7655              47              51  \n",
       "1    0.7766              80              27  \n",
       "2    0.7463              53              51  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_acc_logreg_sig = logreg_fit.score(X_train_sig, y_train_sig).round(4)\n",
    "testing_acc_logreg_sig = logreg_fit.score(X_test_sig, y_test_sig).round(4)\n",
    "auc_score_logreg_sig = roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = logreg_pred).round(4)\n",
    "false_positives_logreg_sig = 53\n",
    "false_negatives_logreg_sig = 51\n",
    "\n",
    "performance_df = performance_df.append(\n",
    "                          {'Model'             : 'Logistic Regression Significant',\n",
    "                          'Training Accuracy'  : training_acc_logreg_sig,\n",
    "                          'Testing Accuracy'   : testing_acc_logreg_sig,\n",
    "                          'AUC Value'          : auc_score_logreg_sig,\n",
    "                          'False Positives'    : false_positives_logreg_sig,\n",
    "                          'False Negatives'    : false_negatives_logreg_sig},\n",
    "                          ignore_index = True)\n",
    "\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The siginificant variables give an acceptable result, not a perfect one however. The false negatives increased which in our case means lost potential coss sales opportunity. Based on our findings in the correlation matrix, combined with the fact that the first split in a tree was IS_FOLLOWER and a value of > 35 % created a homogeneous group of converters, we build the following logistic regression with only 2 variables.\n",
    "- IS_FOLLOWER\n",
    "- IS_TECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7409\n",
      "Testing  ACCURACY: 0.7413\n",
      "AUC Score        : 0.8012\n"
     ]
    }
   ],
   "source": [
    "# train/test split with the 2 selected variables\n",
    "\n",
    "chef_data_short   =  chef_df.loc[ : , ['IS_FOLLOWER','IS_TECH']]\n",
    "chef_target_short =  chef_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "X_train_short, X_test_short, y_train_short, y_test_short = train_test_split(\n",
    "            chef_data_short,\n",
    "            chef_target_short,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 802,\n",
    "            stratify     = chef_target_short)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            max_iter = 1000,\n",
    "                            C = 1,\n",
    "                            random_state = 802)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train_short, y_train_short)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test_short)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(X_train_short, y_train_short).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(X_test_short, y_test_short).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_short,\n",
    "                                          y_score = logreg_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_short,\n",
    "#          pred_y = logreg_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification Tree Tuned</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Tree Pruned</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression Significant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression Short</td>\n",
       "      <td>0.7409</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model Training Accuracy Testing Accuracy  \\\n",
       "0        Classification Tree Tuned            0.8053           0.7988   \n",
       "1       Classification Tree Pruned            0.7951           0.7885   \n",
       "2  Logistic Regression Significant            0.7978           0.7864   \n",
       "3        Logistic Regression Short            0.7409           0.7413   \n",
       "\n",
       "  AUC Value False Positives False Negatives  \n",
       "0    0.7655              47              51  \n",
       "1    0.7766              80              27  \n",
       "2    0.7463              53              51  \n",
       "3    0.8012             121               5  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_acc_logreg_short = logreg_fit.score(X_train_short, y_train_short).round(4)\n",
    "testing_acc_logreg_short = logreg_fit.score(X_test_short, y_test_short).round(4)\n",
    "auc_score_logreg_short = roc_auc_score(y_true  = y_test_short,\n",
    "                                          y_score = logreg_pred).round(4)\n",
    "false_positives_logreg_short = 121\n",
    "false_negatives_logreg_short = 5\n",
    "\n",
    "performance_df = performance_df.append(\n",
    "                          {'Model'             : 'Logistic Regression Short',\n",
    "                          'Training Accuracy'  : training_acc_logreg_short,\n",
    "                          'Testing Accuracy'   : testing_acc_logreg_short,\n",
    "                          'AUC Value'          : auc_score_logreg_short,\n",
    "                          'False Positives'    : false_positives_logreg_short,\n",
    "                          'False Negatives'    : false_negatives_logreg_short},\n",
    "                          ignore_index = True)\n",
    "\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the Hyperparameters of the short logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# GridSearchCV\n",
    "########################################\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "# C_space          = pd.np.arange(0.1, 3.0, 0.1)\n",
    "# warm_start_space = [True, False]\n",
    "# \n",
    "\n",
    "# creating a hyperparameter grid\n",
    "# param_grid = {'C'          : C_space,\n",
    "#               'warm_start' : warm_start_space}\n",
    "# \n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "# lr_tuned = LogisticRegression(solver = 'lbfgs',\n",
    "#                              max_iter = 1000,\n",
    "#                              random_state = 802)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "# lr_tuned_cv = GridSearchCV(estimator  = lr_tuned,\n",
    "#                            param_grid = param_grid,\n",
    "#                            cv         = 3,\n",
    "#                            scoring    = make_scorer(roc_auc_score,\n",
    "#                                                    needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "# lr_tuned_cv.fit(chef_data_short, chef_target_short)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "# print(\"Tuned CV AUC   #    :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the Gridsearch is C: 0.7 and warm_start = True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "# lr_tuned = lr_tuned_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "# lr_tuned_pred = lr_tuned.predict(X_test_short)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "# print('Training ACCURACY:', lr_tuned.score(X_train_short, y_train_short).round(4))\n",
    "# print('Testing  ACCURACY:', lr_tuned.score(X_test_short, y_test_short).round(4))\n",
    "# print('AUC Score        :', roc_auc_score(y_true  = y_test_short,\n",
    "#                                  y_score = lr_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_short,\n",
    "#          pred_y = lr_tuned_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both, the AUC score and the confusion matrix show that there was no improvement of the model based on the tuned Hyperparameters. The AUC score, as well as the false positives and false negatives stay exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we check the performance of additional classification models.\n",
    "\n",
    "Checking the Performance of a Ridge Classifer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7862\n",
      "Testing  ACCURACY: 0.7885\n",
      "AUC Score        : 0.7173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INSTANTIATING a ridge regression model\n",
    "ridge = RidgeClassifier(alpha = 150)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "ridge_fit = ridge.fit(X_train_sig, y_train_sig)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "ridge_pred = ridge_fit.predict(X_test_sig)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', ridge_fit.score(X_train_sig, y_train_sig).round(4))\n",
    "print('Testing  ACCURACY:', ridge_fit.score(X_test_sig, y_test_sig).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = ridge_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the confusion matrix of the ridge classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_sig,\n",
    "#          pred_y = ridge_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge classifier results in \n",
    "- training accuracy: 0.7882\n",
    "- testing accuracy: 0.7864\n",
    "- AUC Score: 0.7226\n",
    "- False Positives: 33\n",
    "- False Negatives: 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification Tree Tuned</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Tree Pruned</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression Significant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression Short</td>\n",
       "      <td>0.7409</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7173</td>\n",
       "      <td>33</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model Training Accuracy Testing Accuracy  \\\n",
       "0        Classification Tree Tuned            0.8053           0.7988   \n",
       "1       Classification Tree Pruned            0.7951           0.7885   \n",
       "2  Logistic Regression Significant            0.7978           0.7864   \n",
       "3        Logistic Regression Short            0.7409           0.7413   \n",
       "4                 Ridge Classifier            0.7862           0.7885   \n",
       "\n",
       "  AUC Value False Positives False Negatives  \n",
       "0    0.7655              47              51  \n",
       "1    0.7766              80              27  \n",
       "2    0.7463              53              51  \n",
       "3    0.8012             121               5  \n",
       "4    0.7173              33              71  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_acc_ridge = ridge_fit.score(X_train_sig, y_train_sig).round(4)\n",
    "testing_acc_ridge = ridge_fit.score(X_test_sig, y_test_sig).round(4)\n",
    "auc_score_ridge = roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = ridge_pred).round(4)\n",
    "false_positives_ridge = 33\n",
    "false_negatives_ridge = 71\n",
    "\n",
    "performance_df = performance_df.append(\n",
    "                          {'Model'             : 'Ridge Classifier',\n",
    "                          'Training Accuracy'  : training_acc_ridge,\n",
    "                          'Testing Accuracy'   : testing_acc_ridge,\n",
    "                          'AUC Value'          : auc_score_ridge,\n",
    "                          'False Positives'    : false_positives_ridge,\n",
    "                          'False Negatives'    : false_negatives_ridge},\n",
    "                          ignore_index = True)\n",
    "\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Performance of a Perceptron Classifier. Instead of testing on the significant variables, we test the Perceptron Classifier on the two variables IS_FOLLOWER and IS_TECH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7409\n",
      "Testing  ACCURACY: 0.7413\n",
      "AUC Score        : 0.8012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INSTANTIATING a Perceptron classification model\n",
    "perceptron = Perceptron()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "perceptron_fit = perceptron.fit(X_train_short, y_train_short)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "perceptron_pred = perceptron_fit.predict(X_test_short)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', perceptron_fit.score(X_train_short, y_train_short).round(4))\n",
    "print('Testing  ACCURACY:', perceptron_fit.score(X_test_short, y_test_short).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_short,\n",
    "                                          y_score = perceptron_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same as with the logistic regression model. Hence, we do not append the performance of the Perceptron classifier\n",
    "\n",
    "The perceptron classifier results in \n",
    "- training accuracy: 0.7409\n",
    "- testing accuracy: 0.7413\n",
    "- AUC Score: 0.8012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Performance of a Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7101\n",
      "Testing  ACCURACY: 0.7187\n",
      "AUC Score        : 0.7914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INSTANTIATING a ridge regression model\n",
    "pac = PassiveAggressiveClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pac_fit = pac.fit(X_train_sig, y_train_sig)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "pac_pred = pac_fit.predict(X_test_sig)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', pac_fit.score(X_train_sig, y_train_sig).round(4))\n",
    "print('Testing  ACCURACY:', pac_fit.score(X_test_sig, y_test_sig).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = pac_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_sig,\n",
    "#          pred_y = pac_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Passive Aggressive Classifier is not able to predict Cross Sell Success. We do not append the performance to our performance comparison dataframe\n",
    "\n",
    "The passive aggressive classifier results in \n",
    "- training accuracy: 0.6792\n",
    "- testing accuracy: 0.6838\n",
    "- AUC Score: 0.5064\n",
    "- False Positives: 0\n",
    "- False Negatives: 154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE can use a KNN Classifier to test for cross_sales_success. \n",
    "\n",
    "First we need to find the optimal number neighbors, using the user defined function \"opt_neighbors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the optimal number of neighbors\n",
    "#opt_neighbors = optimal_neighbors(X_data = X_train_sig, y_data = y_train_sig, response_type = 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns 13 as the optimal number of neighbors. Subsequently we plug n = 13 into the KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7951\n",
      "Testing  ACCURACY: 0.7823\n",
      "AUC Score        : 0.7416\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the data\n",
    "scaler.fit(X_train_sig, y_train_sig)\n",
    "\n",
    "\n",
    "# TRANSFORMING the data\n",
    "X_scaled     = scaler.transform(chef_data_sig)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_scaled_df  = pd.DataFrame(X_scaled) \n",
    "\n",
    "\n",
    "# train-test split with the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "            X_scaled_df,\n",
    "            chef_target_sig,\n",
    "            random_state = 802,\n",
    "            test_size = 0.25,\n",
    "            stratify = chef_target_sig)\n",
    "\n",
    "\n",
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = 13)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(X_train_sig, y_train_sig)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(X_test_sig)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(X_train_sig, y_train_sig).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(X_test_sig, y_test_sig).round(4))\n",
    "      \n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = knn_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test_sig,\n",
    "#          pred_y = knn_pred,\n",
    "#          labels = ['Cross Sell Success', 'Not Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN classifier results in \n",
    "- training accuracy: 0.7855\n",
    "- testing accuracy: 0.7002\n",
    "- AUC Score: 0.5964\n",
    "- False Positives: 48\n",
    "- False Negatives: 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification Tree Tuned</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Tree Pruned</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression Significant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression Short</td>\n",
       "      <td>0.7409</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.7173</td>\n",
       "      <td>33</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7823</td>\n",
       "      <td>0.7416</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model Training Accuracy Testing Accuracy  \\\n",
       "0        Classification Tree Tuned            0.8053           0.7988   \n",
       "1       Classification Tree Pruned            0.7951           0.7885   \n",
       "2  Logistic Regression Significant            0.7978           0.7864   \n",
       "3        Logistic Regression Short            0.7409           0.7413   \n",
       "4                 Ridge Classifier            0.7862           0.7885   \n",
       "5                   KNN Classifier            0.7951           0.7823   \n",
       "\n",
       "  AUC Value False Positives False Negatives  \n",
       "0    0.7655              47              51  \n",
       "1    0.7766              80              27  \n",
       "2    0.7463              53              51  \n",
       "3    0.8012             121               5  \n",
       "4    0.7173              33              71  \n",
       "5    0.7416              48              58  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_acc_knn = knn_fit.score(X_train_sig, y_train_sig).round(4)\n",
    "testing_acc_knn = knn_fit.score(X_test_sig, y_test_sig).round(4)\n",
    "auc_score_knn = roc_auc_score(y_true  = y_test_sig,\n",
    "                                          y_score = knn_pred).round(4)\n",
    "false_positives_knn = 48\n",
    "false_negatives_knn = 58\n",
    "\n",
    "performance_df = performance_df.append(\n",
    "                          {'Model'             : 'KNN Classifier',\n",
    "                          'Training Accuracy'  : training_acc_knn,\n",
    "                          'Testing Accuracy'   : testing_acc_knn,\n",
    "                          'AUC Value'          : auc_score_knn,\n",
    "                          'False Positives'    : false_positives_knn,\n",
    "                          'False Negatives'    : false_negatives_knn},\n",
    "                          ignore_index = True)\n",
    "\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the resulting model performance dataframe\n",
    "#performance_df.to_excel('Performance Summary Final.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result comparison shows that multiple models perform around an AUC value of 0.8\n",
    "With tuning models further, using random forests, etc. I would most likely be able to increase the performance. \n",
    "\n",
    "Given the business context however, my goal is to keep the model actionable and interpretable. As mentioned earlier, it makes sense to optimize for a low number of false negatives. The fewer customers we predict not to buy but that actually buy, the fewer missed opportunities we have. A higher number of false positives surely decreases the efficiency of the promotional campaign but the effectiveness is still high since we catch almost all sales opportunities. \n",
    "\n",
    "Given this reasoning, I select the \"Logistic Regression Short\" as the best model. With an AUC of >0.8 and only 5 False negatives, this is the best fit for the business usecase. The discrepancy between train and test accuracy also does not indicate an overfit model. Another benefit is that this model only has 2 explanatory variables. The insights and recommendations drawn from this are very actionable. Details can be found in my findings summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Logistic Regression Short\n",
      "Training Accuracy                       0.7409\n",
      "Testing Accuracy                        0.7413\n",
      "AUC Value                               0.8012\n",
      "False Positives                            121\n",
      "False Negatives                              5\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Final Model:\n",
    "print(performance_df.loc[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "535px",
    "left": "1147px",
    "right": "20px",
    "top": "56px",
    "width": "233px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
